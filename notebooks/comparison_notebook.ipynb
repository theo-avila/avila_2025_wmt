{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af328eb0-30de-431c-bf2f-7a013e5d0f01",
   "metadata": {},
   "source": [
    "# **Multi-Model Surface Water Mass Transformation Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b98c9-5187-4a77-a10b-b448f68f59cb",
   "metadata": {},
   "source": [
    "## Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc54171-5c38-4a20-8cee-cf63f54231d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esnb import CaseGroup2, NotebookDiagnostic, RequestedVariable\n",
    "from esnb.sites.gfdl import call_dmget\n",
    "\n",
    "mode = \"prod\"\n",
    "verbose = True\n",
    "\n",
    "diag_name = \"Multi-Model Surface Water Mass Transformation Analysis\"\n",
    "diag_desc = \"This is a multi-model notebook for SWMT analysis. \" \\\n",
    "            \"This allows for users to look at surface water mass transformations in average time, monthly, and seasonally\" \\\n",
    "            \" to see where and what fluxes are driving water mass formation or dense water formation in the case of density. \" \\\n",
    "            \" This notebook supports many experiments to allow direct comparison in location and magnitude of swmt.\"\n",
    " \n",
    "# these variables are what are being used to calculate surface water mass transformation in the coupled models\n",
    "# if using ocean only, salinity restoring is required. \n",
    "variables = [\n",
    "    RequestedVariable(\"tos\", \"ocean_monthly\"),\n",
    "    RequestedVariable(\"sos\", \"ocean_monthly\"),\n",
    "    RequestedVariable(\"hfds\", \"ocean_monthly\"),\n",
    "    RequestedVariable(\"wfo\", \"ocean_monthly\"),\n",
    "    RequestedVariable(\"sfdsi\", \"ocean_monthly\"),\n",
    "    RequestedVariable(\"friver\", \"ocean_monthly\"), # salt start\n",
    "    RequestedVariable(\"ficeberg\", \"ocean_monthly\"),\n",
    "    #RequestedVariable(\"fsitherm\", \"ocean_monthly\"),\n",
    "    RequestedVariable(\"evs\", \"ocean_monthly\"),\n",
    "    #RequestedVariable(\"vprec\", \"ocean_monthly\"),\n",
    "    RequestedVariable(\"prsn\", \"ocean_monthly\"),\n",
    "    RequestedVariable(\"prlq\", \"ocean_monthly\"),\n",
    "    RequestedVariable(\"hflso\", \"ocean_monthly\"), # heat start \n",
    "    RequestedVariable(\"hfsso\", \"ocean_monthly\"), \n",
    "    RequestedVariable(\"rlntds\", \"ocean_monthly\"), \n",
    "    RequestedVariable(\"rsntds\", \"ocean_monthly\"), \n",
    "    RequestedVariable(\"siconc\", \"ice\"),\n",
    "]\n",
    "\n",
    "user_options = {\"N/A\": [\"N/A\"]}\n",
    "\n",
    "diag = NotebookDiagnostic(diag_name, diag_desc, variables=variables, **user_options)\n",
    "\n",
    "# supports many experiments, 1 to n\n",
    "groups = [\n",
    "    #CaseGroup(\"odiv-1\", date_range=(\"0200-01-01\", \"0400-12-31\"), verbose=verbose),\n",
    "    #CaseGroup2(\"895\", date_range=(\"0200-01-01\", \"0230-12-31\")),\n",
    "    #CaseGroup2(\"2198\", date_range=(\"0200-01-01\", \"0230-12-31\")),\n",
    "    #CaseGroup2(\"2916\", date_range=(\"0200-01-01\", \"0230-12-31\")),\n",
    "    #CaseGroup2(\"3015\", date_range=(\"0200-01-01\", \"0230-12-31\")),\n",
    "    #CaseGroup2(\"odiv-3\", date_range=(\"0001-01-01\", \"0003-12-31\")),\n",
    "    #CaseGroup2(\"895\", date_range=(\"0101-01-01\", \"0120-12-31\")),\n",
    "    #CaseGroup2(\"2198\", date_range=(\"0101-01-01\", \"0120-12-31\")),\n",
    "    #CaseGroup2(\"2916\", date_range=(\"0101-01-01\", \"0120-12-31\")),\n",
    "    #CaseGroup2(\"3015\", date_range=(\"0101-01-01\", \"0120-12-31\")),\n",
    "    CaseGroup2(\"895\", date_range=(\"0101-01-01\", \"0103-12-31\")),\n",
    "    CaseGroup2(\"2198\", date_range=(\"0101-01-01\", \"0103-12-31\")),\n",
    "    CaseGroup2(\"2916\", date_range=(\"0101-01-01\", \"0103-12-31\")),\n",
    "    CaseGroup2(\"3015\", date_range=(\"0101-01-01\", \"0103-12-31\")),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc93916-0d13-4b24-a395-a5b2703f7ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dmget: All files are online\n"
     ]
    }
   ],
   "source": [
    "diag.resolve(groups)\n",
    "\n",
    "def diag_get():\n",
    "    \"\"\"\n",
    "    helper function to dmget to get dsets of above vars/files\n",
    "    \"\"\"\n",
    "    call_dmget(diag.files)\n",
    "    diag.open()\n",
    "diag_get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62cb5c-f151-427a-add9-9fadd1a696b9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29149e78-5b82-4ee9-ac1d-85af24e54ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xwmt # This is xwmt-new\n",
    "import numpy as np\n",
    "import os\n",
    "import cftime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import momgrid as mg\n",
    "from momgrid.geoslice import geoslice\n",
    "import os as os\n",
    "import cartopy.crs as ccrs\n",
    "import gsw  # TEOS-10\n",
    "import xbudget\n",
    "import xgcm\n",
    "from cmip_basins.basins import generate_basin_codes\n",
    "import copy\n",
    "import cartopy.feature as cfeature\n",
    "import cmocean\n",
    "import pandas as pd\n",
    "from matplotlib.lines import Line2D\n",
    "from collections.abc import Sequence\n",
    "\n",
    "os.environ[\"MOMGRID_WEIGHTS_DIR\"] = \"/nbhome/jpk/grid_weights\" # get rid of any momgrid\n",
    "variables_names = [variable.varname for variable in variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9c8da-71f3-4982-97ce-8072c3cc510b",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1362ce0d-f7ff-40dc-8354-401a99f81c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def realm_list():\n",
    "    \"\"\"\n",
    "    returns dsets[i][j], s.t. i is the case experiment with all variables, and j is the realm\n",
    "    \"\"\"\n",
    "    diag_get()\n",
    "    out = []\n",
    "    ice_var = next((v for v in variables if v.varname == \"siconc\"), None)\n",
    "\n",
    "    for g in diag.groups:\n",
    "        d = g.datasets\n",
    "        realms = []\n",
    "        nonice = [d[v] for v in variables if v is not ice_var]\n",
    "        if nonice:\n",
    "            realms.append(xr.merge(nonice).sortby(\"time\").dropna(dim=\"time\", how=\"all\"))\n",
    "\n",
    "        if ice_var:\n",
    "            realms.append(d[ice_var])\n",
    "\n",
    "        out.append(realms)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def groupCoordinateChange(raw_dsets, groups, basin_integer=None):\n",
    "    \"\"\"\n",
    "    Given some case groups list as groups, performs appropriate\n",
    "    remappings of variable names to be compatible to xwmt python package\n",
    "    maps:\n",
    "        xh->x\n",
    "        yh->y\n",
    "        geolat->lat\n",
    "        geolon->lon\n",
    "    inputs\n",
    "    - dsets[n][i] which is indexed by n as the CaseGroup n, and i as the realm of data (i.e. ocean_month, atmos_cmip, etc.)\n",
    "    - groups as list of CaseGroup objects corresponding to the\n",
    "    - basin_integer\n",
    "    outputs\n",
    "    - dsets[n][i] as the list of lists xarray dataset experiments\n",
    "    - basincodes_arr[n][i] as the list of lists basincodes (i.e. some integer for each basin)\n",
    "    \"\"\"\n",
    "    dsets_copy = copy.deepcopy(raw_dsets)\n",
    "    basincodes_arr = [[None for _ in row] for row in dsets_copy]\n",
    "    coords = {\n",
    "        \"X\": {\n",
    "            \"center\": \"x\",\n",
    "        },\n",
    "        \"Y\": {\n",
    "            \"center\": \"y\",\n",
    "        },\n",
    "    }\n",
    "    metrics = {(\"X\", \"Y\"): \"areacello\"}\n",
    "    for n, group in enumerate(groups):\n",
    "        for i, ds in enumerate(dsets_copy[n]):\n",
    "            dsets_copy[n][i] = mg.Gridset(ds).data\n",
    "            # this must be made more agnostic\n",
    "            if \"y\" not in list(dsets_copy[n][i].coords):\n",
    "                dsets_copy[n][i] = dsets_copy[n][i].rename({\"yh\": \"y\"})\n",
    "            if \"x\" not in list(dsets_copy[n][i].coords):\n",
    "                dsets_copy[n][i] = dsets_copy[n][i].rename({\"xh\": \"x\"})\n",
    "            if \"lat\" not in list(dsets_copy[n][i].coords):\n",
    "                dsets_copy[n][i] = dsets_copy[n][i].rename({\"geolat\": \"lat\"})\n",
    "            if \"lon\" not in list(dsets_copy[n][i].coords):\n",
    "                dsets_copy[n][i] = dsets_copy[n][i].rename({\"geolon\": \"lon\"})\n",
    "            if not \"sfdsi\" in ds:\n",
    "                print(\"sfdsi is missing: Add all-zero field for sfdsi based on hfds\")\n",
    "                dsets_copy[n][i][\"sfdsi\"] = xr.zeros_like(\n",
    "                    dsets_copy[n][i][\"hfds\"]\n",
    "                ).rename(\"sfdsi\")\n",
    "                dsets_copy[n][i][\"sfdsi\"].attrs = {}\n",
    "            if basin_integer is not None:\n",
    "                basincodes_arr[n][i] = generate_basin_codes(\n",
    "                    dsets_copy[n][i], lon=\"lon\", lat=\"lat\", mask=\"wet\"\n",
    "                )\n",
    "                if basin_integer > 10:\n",
    "                    # if greater than 10, this mask is a custom mask, which must be created, not from cmip basins\n",
    "                    basincodes_arr[n][i] = custom_basins(\n",
    "                        basin_integer, basincodes_arr[n][i]\n",
    "                    )\n",
    "                    dsets_copy[n][i] = (\n",
    "                        dsets_copy[n][i]\n",
    "                        .where((basincodes_arr[n][i] == basin_integer))\n",
    "                        .copy(deep=True)\n",
    "                    )\n",
    "                else:\n",
    "                    dsets_copy[n][i] = (\n",
    "                        dsets_copy[n][i]\n",
    "                        .where((basincodes_arr[n][i] == basin_integer))\n",
    "                        .copy(deep=True)\n",
    "                    )\n",
    "            dsets_copy[n][i] = xgcm.Grid(\n",
    "                dsets_copy[n][i],\n",
    "                coords=coords,\n",
    "                metrics=metrics,\n",
    "                boundary={\"X\": \"periodic\", \"Y\": \"extend\"},\n",
    "                autoparse_metadata=False,\n",
    "            )\n",
    "    return dsets_copy, basincodes_arr\n",
    "\n",
    "\n",
    "def custom_basins(basin_integer, basincodes_i):\n",
    "    \"\"\"\n",
    "    given a basin_code integer, returns the custom basin.\n",
    "    Supports N ATL (11), S ATL (12), N PAC (13), S PAC (14), Labrador Sea (15), barents sea (16)\n",
    "    inputs\n",
    "    - basin_integer as the integer corresponding to the basin we care about\n",
    "    - basincodes_i as the basin codes array that is generated\n",
    "    outputs\n",
    "    - copy_arr_XXX(X) as the basin mask of interest\n",
    "    \"\"\"\n",
    "    copy_arr = basincodes_i.copy(deep=True)\n",
    "    # if atlantic if ...\n",
    "    if (basin_integer == 11) or (basin_integer == 12) or (basin_integer == 15):\n",
    "        copy_arr_ATL = copy_arr.where((copy_arr == 2))\n",
    "        copy_arr_ATL[\"avg_lat\"] = copy_arr_ATL.lat.mean(axis=1)\n",
    "        copy_arr_ATL[\"avg_lon\"] = copy_arr_ATL.lon.mean(axis=0)\n",
    "        if (basin_integer == 11) or (basin_integer == 15):\n",
    "            # just NATL basins\n",
    "            copy_arr_NATL = copy_arr_ATL.where(copy_arr_ATL.avg_lat > 0, np.nan)\n",
    "            copy_arr_NATL = copy_arr_NATL.where(np.isnan(copy_arr_NATL), other=11)\n",
    "            if basin_integer == 11:\n",
    "                return copy_arr_NATL\n",
    "            elif basin_integer == 15:\n",
    "                copy_arr_LAB = copy_arr_NATL.where(\n",
    "                    copy_arr_ATL.avg_lat > 54, np.nan\n",
    "                ).copy(deep=True)\n",
    "                copy_arr_LAB = copy_arr_LAB.where(copy_arr_ATL.avg_lon < -47.5, np.nan)\n",
    "                copy_arr_LAB = copy_arr_LAB.where(copy_arr_ATL.avg_lon > -67, np.nan)\n",
    "                copy_arr_LAB = copy_arr_LAB.where(np.isnan(copy_arr_LAB), other=15)\n",
    "                return copy_arr_LAB\n",
    "        elif basin_integer == 12:\n",
    "            # south atlantic basins\n",
    "            copy_arr_SATL = copy_arr_ATL.where(copy_arr_ATL.avg_lat <= 0, np.nan)\n",
    "            copy_arr_SATL = copy_arr_SATL.where(np.isnan(copy_arr_SATL), other=12)\n",
    "            return copy_arr_SATL\n",
    "\n",
    "    # if pacficic ...\n",
    "    elif (basin_integer == 13) or (basin_integer == 14):\n",
    "        copy_arr_PAC = copy_arr.where((copy_arr == 3))\n",
    "        copy_arr_PAC[\"avg_lat\"] = copy_arr_PAC.lat.mean(axis=1)\n",
    "        if basin_integer == 13:\n",
    "            copy_arr_NPAC = copy_arr_PAC.where(copy_arr_PAC.avg_lat > 0, np.nan)\n",
    "            copy_arr_NPAC = copy_arr_NPAC.where(np.isnan(copy_arr_NPAC), other=13)\n",
    "            return copy_arr_NPAC\n",
    "        elif basin_integer == 14:\n",
    "            copy_arr_SPAC = copy_arr_PAC.where(copy_arr_PAC.avg_lat <= 0, np.nan)\n",
    "            copy_arr_SPAC = copy_arr_SPAC.where(np.isnan(copy_arr_SPAC), other=14)\n",
    "            return copy_arr_SPAC\n",
    "    elif (basin_integer == 16):\n",
    "        # barents / nordic\n",
    "        lat2d, lon2d = xr.broadcast(copy_arr['lat'], copy_arr['lon'])\n",
    "        mask_barents = (\n",
    "            (lat2d >= 68) & (lat2d <= 82) &\n",
    "            (lon2d >= 10) & (lon2d <= 60)\n",
    "        )\n",
    "        \n",
    "        barents = copy_arr.where(mask_barents, np.nan)\n",
    "        barents = barents.where(np.isnan(barents), other=16)\n",
    "        return barents\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_budgets_dict(cp=3992.0, rho_ref=1035.0):\n",
    "    \"\"\"\n",
    "    returns a budget dictionary\n",
    "    inputs:\n",
    "    - cp      = 3992.0\n",
    "    - rho_ref = 1035.0\n",
    "    outputs\n",
    "    - budgets_dict\n",
    "    \"\"\"\n",
    "    budgets_dict = {\n",
    "        \"mass\": {},\n",
    "        \"heat\": {\"surface_lambda\": \"tos\"},\n",
    "        \"salt\": {\"surface_lambda\": \"sos\"},\n",
    "    }\n",
    "\n",
    "    budgets_dict[\"heat\"][\"rhs\"] = {\n",
    "        \"var\": None,\n",
    "        \"sum\": {\n",
    "            \"var\": None,\n",
    "            \"latent\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"tracer_content_tendency_per_unit_area\": \"hflso\",\n",
    "                    \"area\": \"areacello\"\n",
    "                }\n",
    "            },\n",
    "            \"sensible\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"tracer_content_tendency_per_unit_area\": \"hfsso\",\n",
    "                    \"area\": \"areacello\"\n",
    "                }\n",
    "            },\n",
    "            \"longwave\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"tracer_content_tendency_per_unit_area\": \"rlntds\",\n",
    "                    \"area\": \"areacello\"\n",
    "                }\n",
    "            },\n",
    "            \"shortwave\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"tracer_content_tendency_per_unit_area\": \"rsntds\",\n",
    "                    \"area\": \"areacello\"\n",
    "                        }\n",
    "                    },\n",
    "            \"hfds_term\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"tracer_content_tendency_per_unit_area\": \"hfds\",\n",
    "                    \"area\": \"areacello\"\n",
    "                }\n",
    "            },\n",
    "            \"surface_exchange_flux_advective_heat\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"specific_heat_capacity\": cp,\n",
    "                    \"lambda_mass\": \"tos\",\n",
    "                    \"mass_density_tendency\": \"wfo\",\n",
    "                    \"area\": \"areacello\",\n",
    "                }\n",
    "            },\n",
    "            \"surface_ocean_flux_advective_heat\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"sign\": -1.0,\n",
    "                    \"specific_heat_capacity\": cp,\n",
    "                    \"lambda_mass\": \"tos\",\n",
    "                    \"mass_density_tendency\": \"wfo\",\n",
    "                    \"area\": \"areacello\",\n",
    "                },  # this just cancels with the surface_exchange flux advective so sign should just be opposite of that (mom6 convention)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    budgets_dict[\"salt\"][\"rhs\"] = {\n",
    "        \"var\": None,\n",
    "        \"sum\": {\n",
    "            \"var\": None,\n",
    "            \"surface_exchange_flux_advective\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"unit_conversion\": 0.001,\n",
    "                    \"lambda_mass\": 0.0,\n",
    "                    \"mass_density_tendency\": \"wfo\",\n",
    "                    \"area\": \"areacello\",\n",
    "                },\n",
    "            },\n",
    "            \"surface_exchange_flux_nonadvective\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"tracer_content_tendency_per_unit_area\": \"sfdsi\",\n",
    "                    \"area\": \"areacello\",\n",
    "                    \"sign\": 1,\n",
    "                },\n",
    "            },\n",
    "            \"wfo_term\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"sign\": -1.0,\n",
    "                    \"unit_conversion\": 0.001,\n",
    "                    \"lambda_mass\": \"sos\",\n",
    "                    \"mass_tendency_per_unit_area\": \"wfo\",\n",
    "                    \"area\": \"areacello\",\n",
    "                },\n",
    "            },\n",
    "            \"rain_and_ice\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"sign\": -1.0,\n",
    "                    \"unit_conversion\": 0.001,\n",
    "                    \"lambda_mass\": \"sos\",\n",
    "                    \"mass_tendency_per_unit_area\": \"prlq\",\n",
    "                    \"area\": \"areacello\",\n",
    "                },\n",
    "            },\n",
    "            \"snow\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"sign\": -1.0,\n",
    "                    \"unit_conversion\": 0.001,\n",
    "                    \"lambda_mass\": \"sos\",\n",
    "                    \"mass_tendency_per_unit_area\": \"prsn\",\n",
    "                    \"area\": \"areacello\",\n",
    "                },\n",
    "            },\n",
    "            \"evaporation\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"sign\": -1.0,\n",
    "                    \"unit_conversion\": 0.001,\n",
    "                    \"lambda_mass\": \"sos\",\n",
    "                    \"mass_tendency_per_unit_area\": \"evs\",\n",
    "                    \"area\": \"areacello\",\n",
    "                },\n",
    "            },\n",
    "            \"rivers\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"sign\": -1.0,\n",
    "                    \"unit_conversion\": 0.001,\n",
    "                    \"lambda_mass\": \"sos\",\n",
    "                    \"mass_tendency_per_unit_area\": \"friver\",\n",
    "                    \"area\": \"areacello\",\n",
    "                },\n",
    "            },\n",
    "            \"icebergs\": {\n",
    "                \"var\": None,\n",
    "                \"product\": {\n",
    "                    \"var\": None,\n",
    "                    \"sign\": -1.0,\n",
    "                    \"unit_conversion\": 0.001,\n",
    "                    \"lambda_mass\": \"sos\",\n",
    "                    \"mass_tendency_per_unit_area\": \"ficeberg\",\n",
    "                    \"area\": \"areacello\",\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "    return budgets_dict, cp, rho_ref\n",
    "\n",
    "\n",
    "def lstr_helper(lstr):\n",
    "    \"\"\"\n",
    "    helper function to give lmin, lmax, dl for various lambdas\n",
    "    inputs\n",
    "    - lstr, which is the lambda string for whatever surface we care about\n",
    "    outputs\n",
    "    - lmin - the maximum lambda value\n",
    "    - lmax - the minimum lambda value\n",
    "    - dl - the delta l for our lambda, defining the bin size\n",
    "    \"\"\"\n",
    "    valid_lambda_list = [\"sigma0\", \"sigma2\", \"theta\", \"salt\"]\n",
    "    if lstr == \"sigma0\":\n",
    "        lmin = 5  # 10\n",
    "        lmax = 30\n",
    "        dl = 0.1  # .1\n",
    "    elif lstr == \"sigma2\":\n",
    "        lmin = 0  # 10\n",
    "        lmax = 40\n",
    "        dl = 0.1\n",
    "    elif lstr == \"theta\":\n",
    "        lmin = -2\n",
    "        lmax = 30\n",
    "        dl = 0.5\n",
    "    elif lstr == \"salt\":\n",
    "        lmin = 20\n",
    "        lmax = 40\n",
    "        dl = 0.1\n",
    "    else:\n",
    "        raise Exception(\n",
    "            f\"invalid lambda value for the surface, please choose a supported lambda from {valid_lambda_list}\"\n",
    "        )\n",
    "    return lmin, lmax, dl\n",
    "\n",
    "\n",
    "def auto_density_bounds(Gm_list, buffer=0.2, threshold_fraction=0.05):\n",
    "    \"\"\"\n",
    "    plotting helper function so the average density plots capture the largest features\n",
    "    \"\"\"\n",
    "    density_dim = f\"{lstr}_l_target\"\n",
    "    combined_total = sum(abs(Gm[\"total\"]) for Gm in Gm_list)\n",
    "    max_transform = combined_total.max()\n",
    "    threshold = max_transform * threshold_fraction\n",
    "    significant = combined_total.where(combined_total >= threshold, drop=True)\n",
    "    lmin, lmax = (\n",
    "        significant[density_dim].min().item(),\n",
    "        significant[density_dim].max().item(),\n",
    "    )\n",
    "    dl = lmax - lmin\n",
    "    lmin, lmax = lmin - dl * buffer, lmax + dl * buffer\n",
    "    return lmin, lmax, dl\n",
    "\n",
    "\n",
    "def G_helper(dset_i, lmin, lmax, dl):\n",
    "    \"\"\"\n",
    "    given some dset_i with previous cleaning, return G with most fields, and swt_processes which are the important fluxes for swmt\n",
    "    inputs\n",
    "    - dsets_i, some dataset with cleaning, should only contain realms not models\n",
    "    - lmin as minimum lambda value\n",
    "    - lmax as maximum lambda value\n",
    "    - dl as some delta in lambda which defines bin size\n",
    "    outputs\n",
    "    - G water mass transformation with variables as the contribution, with the sum of a few as the total\n",
    "    - swmt_processes as the important terms which get summed for total\n",
    "    \"\"\"\n",
    "    case_experiment_0 = dset_i\n",
    "    budgets_dict, cp, rho_ref = get_budgets_dict()\n",
    "    xbudget.collect_budgets(case_experiment_0, budgets_dict)\n",
    "    simple_budget = xbudget.aggregate(budgets_dict)\n",
    "    swmt = xwmt.WaterMassTransformations(case_experiment_0, simple_budget)\n",
    "    swmt_processes = sorted(swmt.available_processes())\n",
    "    dsG = swmt.integrate_transformations(\n",
    "        lstr, bins=np.arange(lmin, lmax, dl), sum_components=True\n",
    "    )\n",
    "    G = dsG.load()\n",
    "    \n",
    "    G[\"salt_term_sum\"] = (\n",
    "        G[\"rain_and_ice\"]\n",
    "        + G[\"snow\"]\n",
    "        + G[\"evaporation\"]\n",
    "        + G['rivers']\n",
    "        + G['icebergs']\n",
    "        + G['surface_exchange_flux_nonadvective_salt']\n",
    "        + G['surface_exchange_flux_advective_salt']\n",
    "    )\n",
    "    G[\"heat_term_sum\"] = (\n",
    "        + G[\"latent\"]\n",
    "        + G[\"sensible\"]\n",
    "        + G[\"longwave\"]\n",
    "        + G[\"shortwave\"]\n",
    "    )\n",
    "    G[\"total\"] = (\n",
    "        G[\"heat_term_sum\"]\n",
    "        + G[\"salt_term_sum\"]\n",
    "    )\n",
    "    return G, swmt_processes, swmt\n",
    "\n",
    "\n",
    "def swmt_average_transformation(\n",
    "    dsets_copy, budgets_dict, cp, rho_ref, region=None, lstr=\"sigma0\", rho_wmt_time='average'\n",
    "):\n",
    "    \"\"\"\n",
    "    given some dset_i calculates the swmt for a few choice regions, & plots values of interest\n",
    "    inputs\n",
    "    - dsets_copy, all case_experiments with ALL realms\n",
    "    - budgets_dict (needs documentation)\n",
    "    - cp heat capacity\n",
    "    - rho_ref reference in situ(?) density, which get converted to potential in swt\n",
    "    - region, the region we care about, where wmt is being carried out\n",
    "    - lstr, lambda str\n",
    "    - rho_wmt_time as the time type for plot (average or seasonal)\n",
    "    outputs\n",
    "    - swmt_list\n",
    "    - max_deltah_bin\n",
    "    - max_deltas_bin\n",
    "    - max_amp_loc_list\n",
    "    - min_amp_loc_list\n",
    "    \"\"\"\n",
    "    lmin_real, lmax_real, dl_real = lstr_helper(lstr)\n",
    "    base_group_label = None\n",
    "    if groups and len(groups) == 1:\n",
    "        base_group_label = groups[0].name\n",
    "        \n",
    "    swmt_all = []\n",
    "    labels_all = []\n",
    "    Gm_all = []\n",
    "    max_amp_idx_all = [] \n",
    "    min_amp_idx_all = []\n",
    "    for exp_i, dset_experiment in enumerate(dsets_copy):\n",
    "        # this goes through each experiment, where we need to again go through the BB sampling\n",
    "        swmt_list = []\n",
    "        Gm_list = []\n",
    "        labels_list = []\n",
    "        max_amp_idx_list = []\n",
    "        min_amp_idx_list = []\n",
    "        # this below loop will now go through each BB sample, and do WMT\n",
    "        for mc_i, ds_bb_i in enumerate(dset_experiment):\n",
    "            G, _, swmt = G_helper(ds_bb_i, lmin_real, lmax_real, dl_real)\n",
    "            swmt_list.append(swmt)\n",
    "            if rho_wmt_time == 'seasonal':\n",
    "                Gm = (G * -1e-9).groupby(\"time.season\").mean(\"time\")\n",
    "                Gm = Gm.sel(season=[\"DJF\",\"MAM\",\"JJA\",\"SON\"])\n",
    "            elif rho_wmt_time == 'average':\n",
    "                Gm = (G * -1e-9).mean(\"time\")\n",
    "            else: \n",
    "                print('invalid time chunking; calculating average transformation')\n",
    "                Gm = (G * -1e-9).mean(\"time\")\n",
    "                \n",
    "            Gm_list.append(Gm)\n",
    "\n",
    "            if groups and len(groups) == len(dsets_copy):\n",
    "                lbl = f\"{groups[exp_i].name}\"\n",
    "            elif base_group_label:\n",
    "                lbl = f\"{base_group_label}\"\n",
    "            else:\n",
    "                lbl = f\"MC_sample_{mc_i+1}\"\n",
    "            # calculate idx of max transformation \n",
    "            labels_list.append(lbl)\n",
    "            max_amp_idx = float(Gm[\"total\"].idxmax(dim=lstr + \"_l_target\"))\n",
    "            max_amp_idx_list.append(max_amp_idx)\n",
    "            min_amp_idx = float(Gm[\"total\"].idxmin(dim=lstr + \"_l_target\"))\n",
    "            min_amp_idx_list.append(min_amp_idx)\n",
    "            \n",
    "        # this appends a list to the list of these items. I.e. this makes nested list so we can index by experiment and BB item\n",
    "        max_amp_idx_all.append(max_amp_idx_list)\n",
    "        min_amp_idx_all.append(min_amp_idx_list)\n",
    "        swmt_all.append(swmt_list)\n",
    "        labels_all.append(labels_list)\n",
    "        Gm_all.append(Gm_list)\n",
    "\n",
    "    lmin, lmax, dl = auto_density_bounds(Gm_list)\n",
    "    terms = {\n",
    "        \"total\":                             \"Total\",\n",
    "        \"heat_term_sum\":                     \"Heat Term Sum\",\n",
    "        #\"hfds_term\":                         \"hfds Term\",\n",
    "        \"salt_term_sum\":                     \"Salt Term Sum\",\n",
    "        #\"wfo_term\":                          \"wfo Term\",\n",
    "        # heat components\n",
    "        \"latent\":                          \"Latent\",\n",
    "        \"sensible\":                        \"Sensible\",\n",
    "        \"longwave\":                        \"Longwave\",\n",
    "        \"shortwave\":                       \"Shortwave\",\n",
    "    \n",
    "        # salt components\n",
    "        \"surface_exchange_flux_nonadvective_salt\":   \"Surface Exchange Flux Nonadvective\",\n",
    "        \"rain_and_ice\":                         \"Rain and Ice\",\n",
    "        \"snow\":                                 \"Snow\",\n",
    "        \"evaporation\":                          \"Evaporation\",\n",
    "        \"rivers\":                               \"Rivers\",\n",
    "        \"icebergs\":                             \"Icebergs\",\n",
    "    }\n",
    "    ymins, ymaxs = [], []\n",
    "    for key in terms:\n",
    "        for Gm_list in Gm_all:\n",
    "            stack = xr.concat([Gm[key] for Gm in Gm_list], dim=\"ensemble\")\n",
    "            ymins.append(float(stack.min()))\n",
    "            ymaxs.append(float(stack.max()))\n",
    "    ymin, ymax = min(ymins), max(ymaxs)\n",
    "\n",
    "    padding = 0.05 * (ymax - ymin)\n",
    "\n",
    "    for key, title in terms.items():\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        ax.axhline(y=0, alpha=.4, linewidth=1, color='gray')\n",
    "        for idx, Gm_list in enumerate(Gm_all):\n",
    "            stack = xr.concat([Gm[key] for Gm in Gm_list], dim=\"ensemble\")\n",
    "            mean = stack.mean(\"ensemble\")\n",
    "            lower = stack.min(\"ensemble\")\n",
    "            upper = stack.max(\"ensemble\")\n",
    "            x = mean[f\"{lstr}_l_target\"]\n",
    "            label_static = labels_all[idx][0]\n",
    "\n",
    "            ax.plot(x, mean.values, lw=2, label=f\"{label_static}\")\n",
    "            if len(Gm_list) > 1:\n",
    "                ax.fill_between(\n",
    "                    x,\n",
    "                    lower.values,\n",
    "                    upper.values,\n",
    "                    alpha=0.3,\n",
    "                    #label=f\"{title} Range {label_static}\",\n",
    "                )\n",
    "    \n",
    "        ax.legend()\n",
    "    \n",
    "        if groups and len(groups) == 1:\n",
    "            filename_label = groups[0].name\n",
    "        elif groups and len(groups) == len(dsets_copy):\n",
    "            filename_label = \"-\".join(g.name for g in groups)\n",
    "        else:\n",
    "            filename_label = \"ensemble\"\n",
    "    \n",
    "        ax.set(\n",
    "            xlabel=f\"potential density {lstr} [kg m$^{{-3}}$–1000]\",\n",
    "            ylabel=\"WMT rate [$10^{9}$ kg/s]\",\n",
    "            title=f\"{title} region={region}\",\n",
    "        )\n",
    "        # this is hard coded for arctic smwt piC density\n",
    "        ax.set_xlim(20, 30)\n",
    "        ax.set_ylim(-5, 14) \n",
    "        ax.legend(fontsize=8, loc=\"best\")\n",
    "        plt.tight_layout()\n",
    "    \n",
    "        fpath = (\n",
    "            f\"/work/Theo.Avila/wmt_summer25/xwmt_cm5_diagnostics/notebooks/\"\n",
    "            f\"diag_plots/swmt_{title}_{region}_{lstr}_{filename_label}_internal_var_{year_chunk_length}yr_{number_samples_generated}_samples\"\n",
    "        )\n",
    "        #plt.savefig(fpath.replace(\" \", \"\") + \".png\")\n",
    "        plt.show()\n",
    "\n",
    "    max_amp_vals = [float(Gm[\"total\"].max()) for Gm in Gm_list]\n",
    "    min_amp_vals = [float(Gm[\"total\"].min()) for Gm in Gm_list]\n",
    "\n",
    "    return {\n",
    "        \"swmt_list\": swmt_all,\n",
    "        \"max_amp_vals\": max_amp_vals,\n",
    "        \"min_amp_vals\": min_amp_vals,\n",
    "        \"max_amp_loc\": max_amp_idx_all,\n",
    "        \"min_amp_loc\": min_amp_idx_all,\n",
    "        \"name_list\": labels_all,\n",
    "        \"lmax_auto\": lmax,\n",
    "        \"lmin_auto\": lmin,\n",
    "        \"dl_auto\": dl,\n",
    "        \"lmax_real\": lmax_real,\n",
    "        \"lmin_real\": lmin_real,\n",
    "        \"dl_real\": dl_real,\n",
    "    }\n",
    "\n",
    "\n",
    "def projection_map(dictionary_terms):\n",
    "    \"\"\"\n",
    "    inputs\n",
    "    {\"swmt_list\": swmt_list,\n",
    "        \"max_deltas_bin\": max_deltas_bin,\n",
    "        \"max_deltah_bin\": max_deltah_bin,\n",
    "        \"max_amp_loc\":  max_amp_loc_list,\n",
    "        \"min_amp_loc\":  min_amp_loc_list,\n",
    "        \"name_list\": labels\n",
    "    }\n",
    "    outputs\n",
    "    - F_delta as the delta in F for certain density bin\n",
    "    - mask_const as the mask to mask non region (i.e. for plotting)\n",
    "    - name_list as the names of the models, such that model[0] - model[1] is the F delta here\n",
    "    \"\"\"\n",
    "    # lambda bin defining\n",
    "    track_max = True\n",
    "    lmax_real = dictionary_terms[\"lmax_real\"]\n",
    "    lmin_real = dictionary_terms[\"lmin_real\"]\n",
    "    dl_real = dictionary_terms[\"dl_real\"]\n",
    "    # swmt location defining\n",
    "    swmt_list = dictionary_terms[\"swmt_list\"]\n",
    "    max_amp_list = dictionary_terms[\"max_amp_loc\"]\n",
    "    min_amp_loc = dictionary_terms[\"min_amp_loc\"][0]\n",
    "    name_list = dictionary_terms[\"name_list\"]\n",
    "    max_amp_loc = max_amp_list[0][0]\n",
    "    list_F_mean_list = []\n",
    "    for indx_i, experiment_i in enumerate(range(len(swmt_list))):\n",
    "        for indx_j, swmt_i in enumerate(swmt_list[indx_i]):\n",
    "            \n",
    "            name_i = name_list[indx_i][indx_j]\n",
    "            if track_max == True:\n",
    "                max_amp_loc = max_amp_list[indx_i][indx_j]\n",
    "            # tracking max sigma for each member\n",
    "            sigma_bin = max_amp_loc\n",
    "            F = swmt_i.map_transformations(\n",
    "                f\"{lstr}\", bins=np.arange(26.5, 28.6, dl_real), sum_components=False\n",
    "            ).sel({lstr + \"_l_target\": sigma_bin}, method=\"nearest\")\n",
    "            F = F.load()\n",
    "            F_mean = F.mean(\"time\") * -1e-9\n",
    "            F_mean[\"salt_term_sum\"] = (\n",
    "                F_mean[\"rain_and_ice_salt\"]\n",
    "                + F_mean[\"snow_salt\"]\n",
    "                + F_mean[\"evaporation_salt\"]\n",
    "                + F_mean[\"rivers_salt\"]\n",
    "                + F_mean[\"icebergs_salt\"]\n",
    "                + F_mean[\"surface_exchange_flux_nonadvective_salt\"]\n",
    "                + F_mean[\"surface_exchange_flux_advective_salt\"]\n",
    "            )\n",
    "            F_mean[\"heat_term_sum\"] = (\n",
    "                F_mean[\"latent_heat\"]\n",
    "                + F_mean[\"sensible_heat\"]\n",
    "                + F_mean[\"longwave_heat\"]\n",
    "                + F_mean[\"shortwave_heat\"]\n",
    "            )\n",
    "            F_mean[\"total\"] = F_mean[\"salt_term_sum\"] + F_mean[\"heat_term_sum\"]\n",
    "            mask_all = basincodes_arr[0][0].where(\n",
    "                basincodes_arr[0][0] != basin_integer, np.nan\n",
    "            )\n",
    "            mask_const = xr.where(mask_all.notnull(), 1, np.nan)\n",
    "            list_F_mean_list.append(F_mean)\n",
    "\n",
    "    return list_F_mean_list, mask_const, name_list, sigma_bin\n",
    "\n",
    "\n",
    "def rename_regrid(list_F_mean, resolution=0.25):\n",
    "    \"\"\"\n",
    "    given some list of ds, rename coords back to gfdl conventions\n",
    "    inputs\n",
    "    - list_F_mean\n",
    "    outputs\n",
    "    - list_F_mean_regrid\n",
    "    \"\"\"\n",
    "    list_F_mean_output = []\n",
    "    for index in range(len(list_F_mean)):\n",
    "        list_F_mean_i = list_F_mean[index].copy(deep=True)\n",
    "        try:\n",
    "            list_F_mean_i = list_F_mean_i.rename(\n",
    "                {\"x\": \"xh\", \"y\": \"yh\", \"lon\": \"geolon\", \"lat\": \"geolat\"}\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            print(\n",
    "                \"Trying to rename coords that do not exist, Check that x, y, lon, lat exist within your input\"\n",
    "            )\n",
    "        list_F_mean_i = mg.Gridset(list_F_mean_i)\n",
    "        list_F_mean_i = list_F_mean_i.regrid(resolution=resolution)\n",
    "        list_F_mean_output.append(list_F_mean_i)\n",
    "\n",
    "    return list_F_mean_output\n",
    "\n",
    "\n",
    "def create_mc_list(\n",
    "    raw_dsets,\n",
    "    number_samples_generated=1,\n",
    "    year_chunk_length=1,\n",
    "    ordered_chunk=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    given raw_dsets returns a list in the same format of raw_dsets with length n year chunks randomly sampled\n",
    "    only works for 1 realm at the moment, and only the first of the list for dsets\n",
    "    inputs\n",
    "    - raw_dsets: the dsets from the header\n",
    "    - number_samples_generated: how many random mc samples to generate, for each model\n",
    "    - year_chunk_length: the length of the mc samples\n",
    "    outputs\n",
    "    - dsets_mc\n",
    "    \"\"\"\n",
    "    n = len(raw_dsets)\n",
    "    if not isinstance(number_samples_generated, Sequence):\n",
    "        number_samples_generated = [number_samples_generated] * n\n",
    "\n",
    "    if not isinstance(year_chunk_length, Sequence):\n",
    "        year_chunk_length = [year_chunk_length] * n\n",
    "    if not isinstance(ordered_chunk, Sequence):\n",
    "        ordered_chunk = [ordered_chunk] * n\n",
    "        \n",
    "    dsets_mc = [] \n",
    "\n",
    "    for i in range(0, len(raw_dsets)):\n",
    "        row_samples = []\n",
    "        min_year = int(raw_dsets[i].time.dt.year.min().item())\n",
    "        max_year = int(raw_dsets[i].time.dt.year.max().item())\n",
    "        if ordered_chunk[i] == False:\n",
    "            # randomized chunking\n",
    "            if number_samples_generated[i] == 1:\n",
    "                starting_int_list = np.array([min_year])\n",
    "            else:\n",
    "                assert (max_year - min_year) >= year_chunk_length[i]\n",
    "                starting_int_list = np.random.randint(\n",
    "                    min_year,\n",
    "                    max_year - year_chunk_length[i],\n",
    "                    size=number_samples_generated[i],\n",
    "                )\n",
    "\n",
    "        # if ordered chunk (linspace esque), makes a n samples with year_chunk_length\n",
    "        elif ordered_chunk[i] == True:\n",
    "            if number_samples_generated[i] == 1:\n",
    "                starting_int_list = np.array([min_year])\n",
    "            else:\n",
    "                N = number_samples_generated[i]\n",
    "                L = year_chunk_length[i]\n",
    "                if (max_year - min_year + 1) < N * L:\n",
    "                    raise ValueError(\n",
    "                        f\"Not enough years ({max_year-min_year+1}) for {N} chunks of length {L}\"\n",
    "                    )\n",
    "                starting_int_list = min_year + np.arange(N) * L\n",
    "\n",
    "        for count, starting_int_i in enumerate(starting_int_list):\n",
    "            t0 = cftime.DatetimeNoLeap(starting_int_i, 1, 1)\n",
    "            t1 = cftime.DatetimeNoLeap(\n",
    "                starting_int_i + year_chunk_length[i] - 1, 12, 31\n",
    "            )\n",
    "            row_samples.append(raw_dsets[i].sel(time=slice(t0, t1)))\n",
    "\n",
    "        dsets_mc.append(row_samples)\n",
    "    return dsets_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4dea93-a6d0-4389-bfee-48eb3b5a6223",
   "metadata": {},
   "source": [
    "## Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d334a-f534-4f5a-b4a8-499ba8838407",
   "metadata": {},
   "source": [
    "### User Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda2dd3-22c8-46a6-8f97-0b9faa584d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_code_list = [[\"Arctic Ocean\",4]] # list of lists with string of location (not checked) and the basin code integer from cmip basins.\n",
    "lstr='sigma0'                          # lambda string, i.e. the bin we care about\n",
    "budgets_dict, cp, rho_ref = get_budgets_dict() #function for getting the dictionary defined in the functions\n",
    "raw_dsets = realm_list()       # takes esnb to list of lists such that it is [[experiment_i_realm_i, experiment_i_realm_j], [experiment_j_realm_i]]\n",
    "year_chunk_length = 20         # length of WMT calculation, can subset the total length, pass scalar if same value for every experiment or list of lengths\n",
    "number_samples_generated = 1   # supports block bootstrapping of experiment, pass scalar if same value for every experiment or list of number of samples\n",
    "ordered_chunk = True           # True has the samples to be concurrent, if only 1 sample it will start at start date, ... \n",
    "                               # else randomly samples from number_samples using year_chunk_length. ordered chunk = False makes most sense with pi-control \n",
    "rho_wmt_time = 'average'       # 'average' or 'seasonal' to plot wmt density plots\n",
    "map_2d = False                 # True maps to 2d, else just does density, WMT plots\n",
    "net_transformation_keys_2d = ['total', \n",
    "                              'salt_term_sum', \n",
    "                              'heat_term_sum']  # if map_2d we can decide which fields to map back to 2d, in arctic heat term matters more.\n",
    "                                                # if changing this use the budgets_dict to see which fields to pass. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737d67f0-050b-4238-b5f8-8ae3cf4ae30a",
   "metadata": {},
   "source": [
    "### Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f3911-763a-4bc2-8537-2a00c6c9e5ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# puts experiments into format appropriate for block bootstrap sampling\n",
    "ocean_only = [ row[0] for row in raw_dsets ]\n",
    "ice_only = [ row[1] for row in raw_dsets ]\n",
    "\n",
    "# block bootstrap sampling of ice and ocean, if ordered chunk == True this will just subset the years to year_length_chunk.\n",
    "# if no sampling or subsetting needed, just pass the maximum length of the experiments. dsets_mc_ice with ordered_chunk = True makes sense as when we can map\n",
    "# back to 2d the ice years allign with the average transformation years, however if ordered chunk = False it would make more sense to have the same random\n",
    "# samples for ice as for ocean which is not the case\n",
    "dsets_mc_list = create_mc_list(\n",
    "    ocean_only,\n",
    "    number_samples_generated=number_samples_generated,\n",
    "    year_chunk_length=year_chunk_length,\n",
    "    ordered_chunk=ordered_chunk\n",
    ")\n",
    "dsets_mc_ice = create_mc_list(\n",
    "    ice_only,\n",
    "    number_samples_generated=number_samples_generated,\n",
    "    year_chunk_length=year_chunk_length,\n",
    "    ordered_chunk=ordered_chunk\n",
    ")\n",
    "# this runs the average transformation. \n",
    "for region_i, basin_integer in basin_code_list:\n",
    "    dsets_copy, basincodes_arr = groupCoordinateChange(dsets_mc_list, groups, basin_integer=basin_integer)\n",
    "    dictionary_terms = swmt_average_transformation(dsets_copy, budgets_dict, cp, rho_ref, region=region_i, lstr=lstr, rho_wmt_time=rho_wmt_time)\n",
    "    if map_2d == True: \n",
    "        list_F_mean_list, mask_const, name_list, sigma_bin = projection_map(dictionary_terms)\n",
    "        list_F_mean_rg = rename_regrid(list_F_mean_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e22e66c-1ba8-4af6-a4e6-f875d0f91349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4d531-6ad1-4b50-baae-9d59a6822e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0141eaf8-f08b-426f-bd66-acf874362b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_transformation_keys = ['total', 'salt_term_sum', 'heat_term_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243f2a7-4645-4aff-85a1-5bf66933d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_runs = len(list_F_mean_rg)\n",
    "flat_mc_ice = [ds for sublist in dsets_mc_ice for ds in sublist]\n",
    "flat_name_list = [name for sublist in name_list for name in sublist]\n",
    "sigma_bin_locs = dictionary_terms['max_amp_loc']\n",
    "flat_sigma_bin_locs = [bin_i for sublist in sigma_bin_locs for bin_i in sublist]\n",
    "\n",
    "for item in net_transformation_keys:\n",
    "    # create 1 x n_runs subplots\n",
    "    fig, axes = plt.subplots(\n",
    "        1, n_runs,\n",
    "        figsize=(6 * n_runs, 6),\n",
    "        subplot_kw={'projection': ccrs.NorthPolarStereo()},\n",
    "        constrained_layout=True\n",
    "    )\n",
    "    fig.suptitle(f\"{region_i} {item} SWMT\",\n",
    "                 fontsize=18, y=1.05)\n",
    "\n",
    "    last_mesh = None\n",
    "\n",
    "    for idx, list_F_mean in enumerate(list_F_mean_rg):\n",
    "        ax = axes[idx] if n_runs > 1 else axes\n",
    "\n",
    "        ds = flat_mc_ice[idx].copy(deep=True).rename({\"xT\": \"xh\", \"yT\": \"yh\"})\n",
    "        siconc_rg = rename_regrid([ds], resolution=.25)[0]\n",
    "        deptho_1k = rename_regrid([ds], resolution=1)[0]\n",
    "        monthly = siconc_rg.groupby(\"time.month\").mean(\"time\")\n",
    "        march = monthly.sel(month=3)\n",
    "        sept = monthly.sel(month=9)\n",
    "        siext_march = march.where(march['siconc'] > .15)\n",
    "        siext_sept  = sept.where(sept['siconc'] > .15)\n",
    "\n",
    "        tot = list_F_mean[item]\n",
    "        vmax = .0015\n",
    "        vmin = -vmax\n",
    "        \n",
    "        cmap = plt.get_cmap('BrBG')\n",
    "        cmap.set_bad('lightgrey')\n",
    "        mesh = tot.plot(\n",
    "            x='lon', y='lat', ax=ax,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "            add_colorbar=False\n",
    "        )\n",
    "        last_mesh = mesh\n",
    "\n",
    "        ax.contour(sept.lon, sept.lat, sept.siconc,\n",
    "                   levels=[0.15], colors='magenta', linewidths=1.5,\n",
    "                   transform=ccrs.PlateCarree(), zorder=3)\n",
    "        ax.contour(march.lon, march.lat, march.siconc,\n",
    "                   levels=[0.15], colors='black', linewidths=1.5,\n",
    "                   transform=ccrs.PlateCarree(), zorder=3)\n",
    "        ax.contour(deptho_1k.lon, deptho_1k.lat, deptho_1k.deptho,\n",
    "                   levels=[1000.], colors='black', linewidths=0.5,\n",
    "                   transform=ccrs.PlateCarree(), zorder=3)\n",
    "        min_year0 = dsets_mc_list[idx][0].time.min().item().year\n",
    "        max_year1 = dsets_mc_list[idx][0].time.max().item().year\n",
    "        ax.set_title(f\"{flat_name_list[idx]} ({min_year0}-{max_year1}) $\\\\sigma_0$={str(flat_sigma_bin_locs[idx])[:6]}\", fontsize=14)\n",
    "        ax.set_extent([-180, 180, 60, 90], crs=ccrs.PlateCarree())\n",
    "\n",
    "    mar_handle = Line2D([], [], color='black', lw=2, label='March ice edge')\n",
    "    sept_handle= Line2D([], [], color='magenta',   lw=2, label='Sept ice edge')\n",
    "    if n_runs > 1:\n",
    "        axes[0].legend(handles=[mar_handle, sept_handle],\n",
    "                       loc='upper left', frameon=True)\n",
    "    else:\n",
    "        ax.legend(handles=[mar_handle, sept_handle],\n",
    "                       loc='upper left', frameon=True)\n",
    "        \n",
    "    cbar = fig.colorbar(\n",
    "        last_mesh, ax=axes,\n",
    "        orientation='vertical', fraction=0.05, pad=0.02, aspect=20\n",
    "    )\n",
    "    cbar.set_label('SWMT [10⁹ kg/s]', fontsize=14)\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "    #plt.savefig(f\"/work/Theo.Avila/wmt_summer25/xwmt_cm5_diagnostics/notebooks/diag_plots/presentation/\" + \n",
    "    #            f\"swmt_2d_{region_i}_{lstr}_{str(sigma_bin)[:6]}_chunk{year_chunk_length}_numsamples{number_samples_generated}_{idx}_{item}_{flat_name_list}_40_100_yr.png\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2facf00-92cd-4af4-8034-a0d2ec3e0aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
